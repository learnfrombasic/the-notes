{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from utils.logging_config import setup_logger\n",
    "\n",
    "ENDPOINTS = {\n",
    "    'openai': \"\", \n",
    "    \"groq\": \"https://api.groq.com/openai/v1/chat/completions\", \n",
    "    \"samba-nova\": \"https://api.sambanova.ai/v1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with API key from environment variable\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "logger = setup_logger(name='[ICLA]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TagResponse:\n",
    "    \"\"\"JSON response format for tag generation\"\"\"\n",
    "    tags: List[str]\n",
    "\n",
    "@dataclass\n",
    "class SummaryResponse:\n",
    "    \"\"\"JSON response format for class summary generation\"\"\"\n",
    "    summary: str\n",
    "\n",
    "@dataclass\n",
    "class ClassificationResponse:\n",
    "    \"\"\"JSON response format for classification\"\"\"\n",
    "    class_name: str\n",
    "    confidence: float\n",
    "    is_out_of_domain: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianClassModel:\n",
    "    \"\"\"Implements the External Continual Learner (ECL) using Gaussian distributions\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 1536):\n",
    "        self.means = {}  # Class means\n",
    "        self.shared_covariance = np.eye(embedding_dim) * 0.1  # Initialize with small variance\n",
    "        self.class_count = 0\n",
    "        self.min_covar_eigenval = 1e-6  # Minimum eigenvalue for numerical stability\n",
    "        \n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get embedding for a text string using OpenAI's embedding model\"\"\"\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        return np.array(response.data[0].embedding)\n",
    "    \n",
    "    def _get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings for multiple texts\"\"\"\n",
    "        if not texts:\n",
    "            return np.array([])\n",
    "        response = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        return np.array([data.embedding for data in response.data])\n",
    "        \n",
    "    def _stabilize_covariance(self, cov_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ensure covariance matrix is numerically stable\"\"\"\n",
    "        # Add small constant to diagonal for numerical stability\n",
    "        cov_matrix += np.eye(cov_matrix.shape[0]) * self.min_covar_eigenval\n",
    "        \n",
    "        # Ensure symmetry\n",
    "        cov_matrix = (cov_matrix + cov_matrix.T) / 2\n",
    "        \n",
    "        # Ensure positive definiteness through eigenvalue decomposition\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "        eigvals = np.maximum(eigvals, self.min_covar_eigenval)\n",
    "        cov_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "        \n",
    "        return cov_matrix\n",
    "    \n",
    "    def update_class_statistics(self, class_name: str, tags: List[str]):\n",
    "        \"\"\"Update Gaussian statistics for a class\"\"\"\n",
    "        embeddings = self._get_embeddings(tags)\n",
    "        if len(embeddings) == 0:\n",
    "            return\n",
    "            \n",
    "        # Update mean for the class\n",
    "        if class_name not in self.means:\n",
    "            self.means[class_name] = np.mean(embeddings, axis=0)\n",
    "            self.class_count += 1\n",
    "        else:\n",
    "            # Incremental mean update\n",
    "            old_mean = self.means[class_name]\n",
    "            n = len(embeddings)\n",
    "            self.means[class_name] = (old_mean + np.mean(embeddings, axis=0)) / 2\n",
    "        \n",
    "        # Update shared covariance matrix\n",
    "        diff = embeddings - self.means[class_name]\n",
    "        class_cov = (diff.T @ diff) / max(len(tags), 1)\n",
    "        \n",
    "        # Update shared covariance with stability check\n",
    "        if self.class_count > 1:\n",
    "            self.shared_covariance = ((self.class_count - 1) * self.shared_covariance + class_cov) / self.class_count\n",
    "        else:\n",
    "            self.shared_covariance = class_cov\n",
    "            \n",
    "        self.shared_covariance = self._stabilize_covariance(self.shared_covariance)\n",
    "    \n",
    "    def get_top_k_classes(self, query_tags: List[str], k: int) -> tuple[List[str], List[float]]:\n",
    "        \"\"\"Get top k most similar classes using Mahalanobis distance\"\"\"\n",
    "        if not query_tags:\n",
    "            return [], []\n",
    "            \n",
    "        query_embeddings = self._get_embeddings(query_tags)\n",
    "        if len(query_embeddings) == 0:\n",
    "            return [], []\n",
    "            \n",
    "        query_mean = np.mean(query_embeddings, axis=0)\n",
    "        \n",
    "        # Calculate Mahalanobis distance to each class\n",
    "        distances = {}\n",
    "        inv_cov = np.linalg.inv(self.shared_covariance)\n",
    "        \n",
    "        for class_name, class_mean in self.means.items():\n",
    "            diff = query_mean - class_mean\n",
    "            dist = np.sqrt(max(0, diff.T @ inv_cov @ diff))  # Ensure non-negative\n",
    "            distances[class_name] = dist.item()\n",
    "        \n",
    "        # Convert distances to similarity scores (inverse of distance)\n",
    "        similarities = {cls: 1.0 / (dist + 1e-6) for cls, dist in distances.items()}\n",
    "        \n",
    "        # Normalize similarities to [0, 1]\n",
    "        max_sim = max(similarities.values()) + 1e-6\n",
    "        similarities = {cls: sim/max_sim for cls, sim in similarities.items()}\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        sorted_classes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        classes, scores = zip(*sorted_classes[:k])\n",
    "        \n",
    "        return list(classes), list(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InCA:\n",
    "    \"\"\"Implementation of In-context Continual Learning Assisted by an ECL\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 1536):\n",
    "        self.ecl = GaussianClassModel(embedding_dim)\n",
    "        self.class_summaries = {}\n",
    "        self.confidence_threshold = 0.3\n",
    "        \n",
    "    def _calculate_semantic_similarity(self, query: str, class_summary: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between query and class summary\"\"\"\n",
    "        if not class_summary:\n",
    "            return 0.0\n",
    "            \n",
    "        query_emb = self.ecl._get_embedding(query)\n",
    "        summary_emb = self.ecl._get_embedding(class_summary)\n",
    "        \n",
    "        similarity = np.dot(query_emb, summary_emb) / (\n",
    "            np.linalg.norm(query_emb) * np.linalg.norm(summary_emb)\n",
    "        )\n",
    "        \n",
    "        return max(0, (similarity + 1) / 2)  # Normalize to [0, 1]\n",
    "        \n",
    "    def _generate_tags(self, query: str, examples: List[Dict[str, List[str]]]) -> List[str]:\n",
    "        \"\"\"Generate semantic tags using LLMs with JSON mode\"\"\"\n",
    "        prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a tag generator that creates semantic tags for text input. \n",
    "                         Generate 5-10 relevant tags that capture the key concepts and intent.\n",
    "                         Respond in JSON format with a 'tags' array.\"\"\"\n",
    "        }\n",
    "        \n",
    "        query_request = {\n",
    "            \"query\": query,\n",
    "            \"examples\": examples\n",
    "        }\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                prompt,\n",
    "                {\"role\": \"user\", \"content\": json.dumps(query_request)}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get(\"tags\", [])\n",
    "        except:\n",
    "            return []\n",
    "            \n",
    "    def _generate_class_summary(self, class_name: str, examples: List[str]) -> str:\n",
    "        \"\"\"Generate class summary using LLMs with JSON mode\"\"\"\n",
    "        prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a class summarizer that creates concise descriptions.\n",
    "                         Create a clear, specific summary of the class based on examples.\n",
    "                         Respond in JSON format with a 'summary' field.\"\"\"\n",
    "        }\n",
    "        \n",
    "        summary_request = {\n",
    "            \"class_name\": class_name,\n",
    "            \"examples\": examples\n",
    "        }\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                prompt,\n",
    "                {\"role\": \"user\", \"content\": json.dumps(summary_request)}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get(\"summary\", \"\")\n",
    "        except:\n",
    "            return \"\"\n",
    "            \n",
    "    def learn_class(self, class_name: str, examples: List[str], tag_examples: List[Dict[str, List[str]]]):\n",
    "        \"\"\"Learn a new class incrementally\"\"\"\n",
    "        summary = self._generate_class_summary(class_name, examples)\n",
    "        self.class_summaries[class_name] = summary\n",
    "        \n",
    "        all_tags = []\n",
    "        for example in examples:\n",
    "            tags = self._generate_tags(example, tag_examples)\n",
    "            all_tags.extend(tags)\n",
    "        \n",
    "        self.ecl.update_class_statistics(class_name, all_tags)\n",
    "        \n",
    "    def predict(self, query: str, k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Predict class for new query with improved confidence and out-of-domain detection\"\"\"\n",
    "        tags = self._generate_tags(query, [])\n",
    "        top_classes, similarities = self.ecl.get_top_k_classes(tags, k)\n",
    "        \n",
    "        if not top_classes:\n",
    "            return {\n",
    "                \"class_name\": \"unknown\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"is_out_of_domain\": True,\n",
    "                \"candidate_classes\": [],\n",
    "                \"statistical_confidence\": 0.0,\n",
    "                \"semantic_confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Calculate statistical confidence\n",
    "        statistical_confidence = similarities[0] if similarities else 0.0\n",
    "        \n",
    "        # Calculate semantic confidence\n",
    "        max_class = top_classes[0]\n",
    "        class_summary = self.class_summaries.get(max_class, \"\")\n",
    "        semantic_confidence = self._calculate_semantic_similarity(query, class_summary)\n",
    "        \n",
    "        # Combined confidence score\n",
    "        final_confidence = 0.6 * statistical_confidence + 0.4 * semantic_confidence\n",
    "        \n",
    "        # Out-of-domain detection\n",
    "        is_out_of_domain = (semantic_confidence < self.confidence_threshold and \n",
    "                           statistical_confidence < self.confidence_threshold)\n",
    "        \n",
    "        return {\n",
    "            \"class_name\": max_class,\n",
    "            \"confidence\": final_confidence,\n",
    "            \"is_out_of_domain\": is_out_of_domain,\n",
    "            \"candidate_classes\": top_classes,\n",
    "            \"semantic_confidence\": semantic_confidence,\n",
    "            \"statistical_confidence\": statistical_confidence\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Validating Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
